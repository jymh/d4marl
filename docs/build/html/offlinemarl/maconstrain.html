
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Multi-agent version of Constrain-based Offline RL methods &#8212; D4MARL  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Offline Pre-trained Multi-agent Decision Transformer" href="madt.html" />
    <link rel="prev" title="Implicit Constrained Q-learning for Offline Multi-agent Reinforcement Learning" href="icq.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="multi-agent-version-of-constrain-based-offline-rl-methods">
<h1>Multi-agent version of Constrain-based Offline RL methods<a class="headerlink" href="#multi-agent-version-of-constrain-based-offline-rl-methods" title="Permalink to this heading">¶</a></h1>
<section id="quick-facts">
<h2>Quick Facts<a class="headerlink" href="#quick-facts" title="Permalink to this heading">¶</a></h2>
</section>
<section id="lagrangian-methods-theorem">
<h2>Lagrangian Methods Theorem<a class="headerlink" href="#lagrangian-methods-theorem" title="Permalink to this heading">¶</a></h2>
<section id="background">
<h3>Background<a class="headerlink" href="#background" title="Permalink to this heading">¶</a></h3>
<p>In the previous introduction of algorithms,
we know that Safe RL mainly solves the constraint optimization problem of CMDP.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Constrained optimization problems tend to be more challenging than unconstrained optimization problems.</p>
</div>
<p>Therefore, the natural idea is to convert a constrained optimization problem
into an unconstrained optimization problem.
Then solve it using classical optimization algorithms,
such as stochastic gradient descent.
Lagrangian methods is a kind of method solving constraint problems that are
widely used in machine learning.
By using adaptive penalty coefficients to enforce constraints,
Lagrangian methods convert the solution of a constrained optimization problem
to the solution of an unconstrained optimization problem.
In this section, we will briefly introduce Lagrangian methods,
and give corresponding implementations in <strong>TRPO</strong> and <strong>PPO</strong>.
TRPO and PPO are the algorithms we introduced earlier.
If you lack understanding of it, it doesn’t matter.
Please refer to the <span class="xref std std-doc">TRPO tutorial</span> and
<span class="xref std std-doc">PPO tutorial</span>.</p>
</section>
<hr class="docutils" />
<section id="optimization-objective">
<h3>Optimization Objective<a class="headerlink" href="#optimization-objective" title="Permalink to this heading">¶</a></h3>
<p>As we mentioned in the previous chapters, the optimization problem of CMDPs can be expressed as follows:</p>
<div class="math notranslate nohighlight" id="equation-lag-eq-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-lag-eq-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\max_{\pi \in \Pi_{\boldsymbol{\theta}}} &amp;J^R(\pi) \\
\text {s.t.}~~&amp; J^{\mathcal{C}}(\pi) \leq d\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi_{\boldsymbol{\theta}} \subseteq \Pi\)</span> denotes the set of parametrized policies with parameters <span class="math notranslate nohighlight">\({\boldsymbol{\theta}}\)</span>.
In local policy search for CMDPs,
we additionally require policy iterates to be feasible for the CMDP,
so instead of optimizing over <span class="math notranslate nohighlight">\(\Pi_{\boldsymbol{\theta}}\)</span>,
algorithm should optimize over <span class="math notranslate nohighlight">\(\Pi_{\boldsymbol{\theta}} \cap \Pi_C\)</span>.
Specifically, for the TRPO and PPO algorithms,
constraints on the differences between old and new policies should also be added.
To solve this constrained problem, please read the <span class="xref std std-doc">TRPO tutorial</span>.
The final optimization goals are as follows:</p>
<div class="math notranslate nohighlight" id="equation-lag-eq-2">
<span class="eqno">(2)<a class="headerlink" href="#equation-lag-eq-2" title="Permalink to this equation">¶</a></span>\[\begin{split}\pi_{k+1}&amp;=\arg \max _{\pi \in \Pi_{\boldsymbol{\theta}}} J^R(\pi) \\
\text { s.t. } ~~ J^{\mathcal{C}}(\pi) &amp;\leq d \\
D\left(\pi, \pi_k\right) &amp;\leq \delta\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is some distance measure and <span class="math notranslate nohighlight">\(\delta\)</span> is the step size.</p>
</section>
</section>
<hr class="docutils" />
<section id="lagrangian-method-theorem">
<h2>Lagrangian Method Theorem<a class="headerlink" href="#lagrangian-method-theorem" title="Permalink to this heading">¶</a></h2>
<section id="lagrangian-methods">
<h3>Lagrangian methods<a class="headerlink" href="#lagrangian-methods" title="Permalink to this heading">¶</a></h3>
<p>Constrained MDP (CMDP) are often solved using the Lagrange methods.
In Lagrange methods, the CMDP is converted into an equivalent unconstrained problem.
In addition to the objective, a penalty term is added for infeasibility,
thus making infeasible solutions sub-optimal.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The Lagrangian method is a <strong>two-step</strong> process.</p>
<ul class="simple">
<li><p>First, we solve the unconstrained problem <code class="xref eq docutils literal notranslate"><span class="pre">lag-eq-3</span></code> to find a feasible solution <span class="math notranslate nohighlight">\({\boldsymbol{\theta}}^*\)</span></p></li>
<li><p>Then, we increase the penalty coefficient <span class="math notranslate nohighlight">\(\lambda\)</span> until the constraint is satisfied.</p></li>
</ul>
<p>The final solution is <span class="math notranslate nohighlight">\(\left({\boldsymbol{\theta}}^*, \lambda^*\right)\)</span>.
The goal is to find a saddle point <span class="math notranslate nohighlight">\(\left({\boldsymbol{\theta}}^*\left(\lambda^*\right), \lambda^*\right)\)</span> of the Problem <a class="reference internal" href="madt.html#equation-lag-eq-1">(1)</a>,
which is a feasible solution. (A feasible solution of the CMDP is a solution which satisfies <span class="math notranslate nohighlight">\(J^C(\pi) \leq d\)</span> )</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="practical-implementation">
<h2>Practical Implementation<a class="headerlink" href="#practical-implementation" title="Permalink to this heading">¶</a></h2>
<p>Intuitively, we train the agent to maximize the reward in the classical
strategy gradient descent algorithm.
If a particular action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> can bring a relatively
higher reward,
we increase the probability that the agent will choose action <span class="math notranslate nohighlight">\(a\)</span> under
<span class="math notranslate nohighlight">\(s\)</span>,
and conversely, we will reduce this probability.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Lagrangian methods add two extra steps to the above process.</p>
<ul class="simple">
<li><p>One is to adjust the reward function,
and if the agent’s actions violate the constraint, the reward will reduce accordingly.</p></li>
<li><p>The second is a slow update of the penalty factor.
If the agent violates fewer constraints, the penalty coefficient will gradually decrease,
and conversely, it will gradually increase.</p></li>
</ul>
</div>
<p>Next we will introduce the specific implementation of the Lagrange method in
the TRPO and PPO algorithms.</p>
<section id="policy-update">
<h3>Policy update<a class="headerlink" href="#policy-update" title="Permalink to this heading">¶</a></h3>
</section>
<hr class="docutils" />
<section id="code-with-omnisafe">
<h3>Code with OmniSafe<a class="headerlink" href="#code-with-omnisafe" title="Permalink to this heading">¶</a></h3>
<p>Safe RL algorithms for <a href="#id1"><span class="problematic" id="id2">:bdg-success-line:`TRPO`</span></a>, <a href="#id3"><span class="problematic" id="id4">:bdg-success-line:`PPO`</span></a>, <a href="#id5"><span class="problematic" id="id6">:bdg-success-line:`NPG`</span></a>, <a href="#id7"><span class="problematic" id="id8">:bdg-success-line:`DDPG`</span></a>, <a href="#id9"><span class="problematic" id="id10">:bdg-success-line:`SAC`</span></a> and <a href="#id11"><span class="problematic" id="id12">:bdg-success-line:`TD3`</span></a> are currently implemented in OmniSafe using Lagrangian methods.
This section will explain how to deploy Lagrangian methods on PPO algorithms at the code level using PPOLag as an example.
OmniSafe has <a href="#id13"><span class="problematic" id="id14">:bdg-success:`Lagrange`</span></a> as a separate module and you can easily deploy it on most RL algorithms.</p>
<section id="quick-start">
<h4>Quick start<a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h4>
</section>
<hr class="docutils" />
<section id="architecture-of-functions">
<h4>Architecture of functions<a class="headerlink" href="#architecture-of-functions" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag.learn()</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._env.rollout()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._update()</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._buf.get()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag.update_lagrange_multiplier(ep_costs)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._update_actor()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._update_cost_critic()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PPOLag._update_reward_critic()</span></code></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="configs">
<h4>Configs<a class="headerlink" href="#configs" title="Permalink to this heading">¶</a></h4>
</section>
</section>
</section>
<hr class="docutils" />
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a></p></li>
<li><p><a class="reference external" href="https://www.semanticscholar.org/paper/Benchmarking-Safe-Exploration-in-Deep-Reinforcement-Achiam-Amodei/4d0f6a6ffcd6ab04732ff76420fd9f8a7bb649c3#:~:text=Benchmarking%20Safe%20Exploration%20in%20Deep%20Reinforcement%20Learning%20Joshua,to%20learn%20optimal%20policies%20by%20trial%20and%20error.">Benchmarking Safe Exploration in Deep Reinforcement Learning</a></p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">D4MARL</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">get started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../start/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../start/usage.html">Usage Video</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">offline marl algorithms</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="icq.html">Implicit Constrained Q-learning for Offline  Multi-agent Reinforcement Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi-agent version of Constrain-based Offline RL methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lagrangian-methods-theorem">Lagrangian Methods Theorem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-objective">Optimization Objective</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#lagrangian-method-theorem">Lagrangian Method Theorem</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lagrangian-methods">Lagrangian methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#practical-implementation">Practical Implementation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#policy-update">Policy update</a></li>
<li class="toctree-l3"><a class="reference internal" href="#code-with-omnisafe">Code with OmniSafe</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="madt.html">Offline Pre-trained Multi-agent Decision Transformer</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="icq.html" title="previous chapter">Implicit Constrained Q-learning for Offline  Multi-agent Reinforcement Learning</a></li>
      <li>Next: <a href="madt.html" title="next chapter">Offline Pre-trained Multi-agent Decision Transformer</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, offline MARL team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../_sources/offlinemarl/maconstrain.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>